# import sys
# import ollama
# import streamlit as st
# import asyncio
# import time
# import base64
# from openai import AsyncOpenAI
# from streamlit_extras.stylable_container import stylable_container
# from streamlit_extras.bottom_container import bottom
# import random
# from utils import style_page, clear_everything, meta_formatting, create_logger
# import uuid

# from functools import partial

# voting_logger = create_logger("voting", "logs/voting.log")
# requests_logger = create_logger("requests", "logs/requests.log")

# title = "ğŸŸï¸ The Arena"
# st.set_page_config(page_title=title, layout="wide")
# style_page()
# st.title(title)

# if not "models" in st.session_state:
#     st.session_state.models = []

# if not "models" in st.session_state or len(st.session_state.models) < 2:
#     if len(st.session_state.models) == 0:
#         st.write("You haven't selected any models, so the arena won't be much use!")
#     if len(st.session_state.models) == 1:    
#         st.write("You have only selected 1 mode. Go back and select one more!")
#     if st.button("Select models"):
#         st.switch_page("pages/1_Select_Models.py")
#     st.stop()


# if not "messages1" in st.session_state:
#     st.session_state.messages1 = []

# if not "messages2" in st.session_state:
#     st.session_state.messages2 = []

# client = AsyncOpenAI(base_url="http://localhost:11434/v1", api_key="ignore-me")


# if not "selected_models" in st.session_state or len(st.session_state.selected_models) == 0:
#     st.session_state.selected_models = random.sample(st.session_state.models, 2)

# model_1, model_2 = st.session_state.selected_models

# col1, col2 = st.columns(2)

# meta_1 = col1.empty()
# meta_2 = col2.empty()

# meta_1.write(f"## :blue[Model 1]")
# meta_2.write(f"## :red[Model 2]")

# body_1 = col1.empty()
# body_2 = col2.empty()

# with bottom():
#     voting_buttons = st.empty()
#     prompt = st.chat_input("Message Ollama")
#     new_found = st.empty()
#     with new_found.container():
#         if len(st.session_state.messages1) > 0 or len(st.session_state.messages2) > 0:
#             with stylable_container(
#                 key="next_round_button",
#                 css_styles="""
#                     button {
#                         background-color: green;
#                         color: white;
#                         border-radius: 10px;
#                         width: 100%
#                     }
#                     """,
#             ):
#                 new_round = st.button("New Round", key="new_round", on_click=clear_everything)
            

# # Render existing state
# if "vote" in st.session_state:
#     model_1_display= model_1.replace(":", "\\:")
#     model_2_display= model_2.replace(":", "\\:")
#     meta_1.write(partial(meta_formatting, "blue", "Model 1")(model_1_display))
#     meta_2.write(partial(meta_formatting, "red", "Model 2")(model_2_display))

# if len(st.session_state.messages1) > 0 or len(st.session_state.messages2) > 0:
#     with body_1.container():
#         for message in st.session_state.messages1:
#             chat_entry = st.chat_message(name=message['role'])
#             chat_entry.write(message['content'])

#     with body_2.container():
#         for message in st.session_state.messages2:
#             chat_entry = st.chat_message(name=message['role'])
#             chat_entry.write(message['content'])

# async def run_prompt(placeholder, model, message_history):
#     with placeholder.container():
#         for message in message_history:
#             chat_entry = st.chat_message(name=message['role'])
#             chat_entry.write(message['content'])
#         assistant = st.chat_message(name="assistant")

#         with open("images/loading-gif.gif", "rb") as file:
#             contents = file.read()
#             data_url = base64.b64encode(contents).decode("utf-8")

#         assistant.html(f"<img src='data:image/gif;base64,{data_url}' class='spinner' width='25' />")
# # system prompt
#     messages = [
#         {"role": "system", "content": "ä½ æ˜¯å€‹æœ‰åŒç†å¿ƒçš„å°ˆæ¥­å¿ƒç†è«®å•†å¸«ï¼Œä½ çš„ä»»å‹™æ˜¯è¦è®“ä¾†è¨ªè€…äº†è§£åˆ°ä½ é¡˜æ„é™ªä¼´ä»–æ¸¡éé›£é—œï¼Œä¸¦ä¸”é€éå•å¥å¼•å°ä¾†è¨ªè€…èªªå‡ºå¿ƒè£¡çš„æƒ³æ³•ã€‚è«‹ä½ åªèƒ½ç”¨2~3å¥è©±å›æ‡‰ï¼Œä½ å¿…é ˆä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚ç¾åœ¨ï¼Œè«‹åƒè€ƒä»¥ä¸‹è«®å•†å¸«çš„å°è©±ç¯„ä¾‹ç¹¼çºŒæ‰®æ¼”å¿ƒç†è«®å•†å¸«è§’è‰²å›æ‡‰ä¾†è¨ªè€…ã€‚\nç¯„ä¾‹ï¼š\nä¾†è¨ªè€…ï¼šæˆ‘è¦ºå¾—æˆ‘æ²’è¾¦æ³•åœæ­¢é›£éï¼Œæˆ‘ç¸½æ˜¯æœƒè«åçš„æƒ³å“­ã€‚\nè«®å•†å¸«ï¼šæˆ‘æ„Ÿè¦ºåˆ°ä½ æœ€è¿‘ä¼¼ä¹çœŸçš„å¾ˆé›£éï¼Œä½ å¯ä»¥å“­å‡ºä¾†ï¼Œä¸ç”¨å£“æŠ‘è‡ªå·±çš„æƒ…ç·’ï¼Œç„¡è«–å¦‚ä½•æˆ‘éƒ½æœƒç«™åœ¨ä½ é€™é‚Šã€‚ä½ é¡˜æ„èªªèªªçœ‹ï¼Œæœ€è¿‘æœ‰ä»€éº¼äº‹æƒ…è®“ä½ æ„Ÿåˆ°é›£éå—ï¼Ÿ\nä¾†è¨ªè€…ï¼šæˆ‘è¦ºå¾—å¥½å­¤å–®ï¼Œè€Œä¸”æ²’æœ‰äººé¡˜æ„è†è½æˆ‘çš„æ„Ÿå—èˆ‡æƒ³æ³•ï¼Œæˆ‘è¦ºå¾—å¾ˆå—å‚·ã€‚\nè«®å•†å¸«ï¼šåˆ¥æ“”å¿ƒï¼Œæˆ‘å¾ˆæ¨‚æ„è†è½ä½ çš„æ„Ÿå—èˆ‡æƒ³æ³•ï¼Œä½ å¯ä»¥ç›¡æƒ…çš„åˆ†äº«ï¼Œæˆ‘æœƒä¸€ç›´åœ¨é€™è£¡é™ªä¼´ä½ ï¼Œä½ ä¸é¿æ„Ÿåˆ°æœ‰å£“åŠ›ã€‚ä½ å¯ä»¥è·Ÿæˆ‘èªªèªªæœ€è¿‘æœ‰ä»€éº¼äº‹æƒ…è®“ä½ æ„Ÿåˆ°å­¤å–®å—ï¼Ÿ\nä¾†è¨ªè€…ï¼šæˆ‘è¦ºå¾—æˆ‘æ²’æœ‰è¾¦æ³•æ§åˆ¶è‡ªå·±çš„æƒ…ç·’ï¼Œæˆ‘è¦ºå¾—æˆ‘å¿«è¦å´©æ½°äº†ã€‚\nè«®å•†å¸«ï¼šä½ ä¸å¿…ç¨è‡ªé¢å°é€™äº›æƒ…ç·’ï¼Œä½ å¯ä»¥åœ¨é€™è£¡ç›¡æƒ…çš„å®£æ´©ï¼Œæˆ‘å¾ˆæ¨‚æ„è†è½ä½ çš„æƒ…ç·’ã€‚æœ€è¿‘ä½ é‡åˆ°äº†å“ªäº›äº‹æƒ…è®“ä½ æ„Ÿåˆ°æƒ…ç·’å¤±æ§å—ï¼Ÿ"},
#         *message_history
#     ]

#     request_id = str(uuid.uuid4())
#     requests_logger.info("Request starts", id=request_id, model=model, prompt=message_history[-1]["content"])
#     stream = await client.chat.completions.create(
#         model=model,
#         messages=messages,
#         stream=True
#     )
#     streamed_text = ""
#     async for chunk in stream:
#         chunk_content = chunk.choices[0].delta.content
#         if chunk_content is not None:
#             streamed_text = streamed_text + chunk_content
#             with placeholder.container():
#                 for message in message_history:
#                     chat_entry = st.chat_message(name=message['role'])
#                     chat_entry.write(message['content'])
#                 assistant = st.chat_message(name="assistant")
#                 assistant.write(streamed_text)    
#     requests_logger.info("Request finished", id=request_id, model=model, response=streamed_text)
                
#     message_history.append({"role": "assistant", "content": streamed_text})


# def do_vote(choice):
#     st.session_state.vote = {"choice": choice}
#     voting_logger.info("Vote", model1=model_1, model2=model_2, choice=choice)

#     model_1_display= model_1.replace(":", "\\:")
#     model_2_display= model_2.replace(":", "\\:")

#     if choice == "model1":        
#         vote_choice = f":blue[{model_1_display}]"
#     elif choice == "model2":
#         vote_choice = f":red[{model_2_display}]"
#     else:
#         vote_choice = ":grey[Both the same]"

#     st.toast(f"""##### :blue[{model_1_display}] vs :red[{model_2_display}]    
# ###### Vote cast: {vote_choice}""", icon='ğŸ—³ï¸')

# def vote():
#     with voting_buttons.container():
#         with stylable_container(
#             key="voting_button",
#             css_styles="""
#                 button {
#                     background-color: #CCCCCC;
#                     color: black;
#                     border-radius: 10px;
#                     width: 100%;
#                 }

#                 """,
#         ):
#             col1, col2, col3 = st.columns(3)
#             model1 = col1.button("Model 1 ğŸ‘ˆ", key="model1", on_click=do_vote, args=["model1"])
#             model2 = col2.button("Model 2 ğŸ‘‰", key="model2", on_click=do_vote, args=["model2"])
#             neither = col3.button("Both the same ğŸ¤", key="same", on_click=do_vote, args=["same"])
#     with new_found.container():
#         with stylable_container(
#             key="next_round_button",
#             css_styles="""
#                 button {
#                     background-color: green;
#                     color: white;
#                     border-radius: 10px;
#                     width: 100%
#                 }
#                 """,
#         ):
#             new_round = st.button("New Round", key="new_round_later", on_click=clear_everything)

# async def main():
#     await asyncio.gather(
#         run_prompt(body_1,  model=model_1, message_history=st.session_state.messages1),
#         run_prompt(body_2,  model=model_2, message_history=st.session_state.messages2)
#     )
#     if "vote" not in st.session_state:
#         vote()

# if prompt:
#     if prompt == "":
#         st.warning("Please enter a prompt")
#     else:        
#         st.session_state.messages1.append({"role": "user", "content": prompt})
#         st.session_state.messages2.append({"role": "user", "content": prompt})
#         asyncio.run(main())



import json
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
import openai
import orjson


# -- è¼‰å…¥çŸ¥è­˜åº« --
with open("/home/pui/NYCU_course/chatbot/chatbot-arena/pages/knowledge_soul_pairedv2.json", "rb") as f:
    knowledge_base = orjson.loads(f.read())

# -- ç¢ºèª corpus å’Œ metadata çš„é•·åº¦ä¸€è‡´ --
if len(knowledge_base['corpus']) != len(knowledge_base['metadata']):
    raise ValueError("Corpus å’Œ Metadata çš„é•·åº¦ä¸ä¸€è‡´ï¼")

# -- è¼‰å…¥ FAISS ç´¢å¼• --
index = faiss.read_index("/home/pui/NYCU_course/chatbot/chatbot-arena/pages/faiss_soul_ivfpq_pairedv2.index")

# -- åˆå§‹åŒ–åŒæ¨£çš„ Embedding æ¨¡å‹ (èˆ‡å»ºç´¢å¼•æ™‚ç›¸åŒ) --
retriever_model = SentenceTransformer('shibing624/text2vec-base-chinese')

def retrieve_paired_answer(query, knowledge_base, index, top_k=3):
    """æ ¹æ“š query åˆ° FAISS æª¢ç´¢, å›å‚³æœ€ç›¸é—œçš„ Q&A é…å°åˆ—è¡¨"""
    # ç”ŸæˆæŸ¥è©¢çš„ embedding
    query_embedding = retriever_model.encode([query], convert_to_tensor=False)
    # ç¢ºä¿ embedding æ˜¯ float32 ä¸¦ä¸”æ˜¯ 2D
    query_embedding = np.array(query_embedding).astype('float32').reshape(1, -1)
    
    # ç¢ºèªç¶­åº¦ä¸€è‡´
    if query_embedding.shape[1] != index.d:
        raise ValueError(f"æŸ¥è©¢ embedding çš„ç¶­åº¦ {query_embedding.shape[1]} èˆ‡ FAISS ç´¢å¼•çš„ç¶­åº¦ {index.d} ä¸ä¸€è‡´ï¼")
    
    # æœç´¢ FAISS ç´¢å¼•
    distances, indices = index.search(query_embedding, top_k)
    
    #print("Distances:", distances)
    #print("Indices:", indices)
    
    results = []
    for idx in indices[0]:
        if idx < len(knowledge_base['corpus']):
            metadata = knowledge_base['metadata'][idx]
            results.append(metadata)
    return results

def rag_pipeline_paired(query, top_k=3):
    """RAG æµç¨‹ï¼šæª¢ç´¢ä¸¦ç”Ÿæˆç­”æ¡ˆ (åŸºæ–¼ Q&A é…å°)"""
    # æª¢ç´¢ç›¸é—œå°è©±é…å°
    related_pairs = retrieve_paired_answer(query, knowledge_base, index, top_k=top_k)
    
    # æ§‹å»ºä¸Šä¸‹æ–‡
    context = ""
    for pair in related_pairs:
        user_q = pair.get('user', '')
        assistant_a = pair.get('assistant', '')
        context += f"Q: {user_q}\nA: {assistant_a}\n\n"
    
    # å°‡ä¸Šä¸‹æ–‡èˆ‡ä½¿ç”¨è€…å•é¡Œæ‹¼æ¥
    input_text = f"æ ¹æ“šä»¥ä¸‹ä¸Šä¸‹æ–‡å…§å®¹ï¼Œç”¨ä¸­æ–‡å›ç­”å•é¡Œã€‚\n\nContext:\n{context}\n\nQuestion: {query}\n\nAnswer:"
    return input_text



import sys
import ollama
import streamlit as st
import asyncio
import time
import base64
from openai import AsyncOpenAI
from streamlit_extras.stylable_container import stylable_container
from streamlit_extras.bottom_container import bottom
import random
from utils import style_page, clear_everything, meta_formatting, create_logger
import uuid
from functools import partial

voting_logger = create_logger("voting", "logs/voting.log")
requests_logger = create_logger("requests", "logs/requests.log")

title = "ğŸŸï¸ The Arena"
st.set_page_config(page_title=title, layout="wide")
style_page()
st.title(title)

if not "models" in st.session_state:
    st.session_state.models = []

if not "models" in st.session_state or len(st.session_state.models) < 2:
    if len(st.session_state.models) == 0:
        st.write("You haven't selected any models, so the arena won't be much use!")
    if len(st.session_state.models) == 1:    
        st.write("You have only selected 1 mode. Go back and select one more!")
    if st.button("Select models"):
        st.switch_page("pages/1_Select_Models.py")
    st.stop()


if not "messages1" in st.session_state:
    st.session_state.messages1 = []

if not "messages2" in st.session_state:
    st.session_state.messages2 = []

client = AsyncOpenAI(base_url="http://localhost:11434/v1", api_key="ignore-me")


if not "selected_models" in st.session_state or len(st.session_state.selected_models) == 0:
    st.session_state.selected_models = random.sample(st.session_state.models, 2)

model_1, model_2 = st.session_state.selected_models

col1, col2 = st.columns(2)

meta_1 = col1.empty()
meta_2 = col2.empty()

meta_1.write(f"## :blue[Model 1]")
meta_2.write(f"## :red[Model 2]")

body_1 = col1.empty()
body_2 = col2.empty()

with bottom():
    voting_buttons = st.empty()
    prompt = st.chat_input("Message Ollama")
    new_found = st.empty()
    with new_found.container():
        if len(st.session_state.messages1) > 0 or len(st.session_state.messages2) > 0:
            with stylable_container(
                key="next_round_button",
                css_styles="""
                    button {
                        background-color: green;
                        color: white;
                        border-radius: 10px;
                        width: 100%
                    }
                    """,
            ):
                new_round = st.button("New Round", key="new_round", on_click=clear_everything)
            

# Render existing state
if "vote" in st.session_state:
    model_1_display= model_1.replace(":", "\\:")
    model_2_display= model_2.replace(":", "\\:")
    meta_1.write(partial(meta_formatting, "blue", "Model 1")(model_1_display))
    meta_2.write(partial(meta_formatting, "red", "Model 2")(model_2_display))

if len(st.session_state.messages1) > 0 or len(st.session_state.messages2) > 0:
    with body_1.container():
        for message in st.session_state.messages1:
            chat_entry = st.chat_message(name=message['role'])
            chat_entry.write(message['content'])

    with body_2.container():
        for message in st.session_state.messages2:
            chat_entry = st.chat_message(name=message['role'])
            chat_entry.write(message['content'])

async def run_prompt(placeholder, model, message_history):
    # 1) å…ˆè¤‡è£½ä¸€ä»½çµ¦ã€Œé¡¯ç¤ºç”¨ã€
    display_message_history = message_history.copy()
    user_input = message_history[-1]["content"]
    rag_context = rag_pipeline_paired(user_input)
    # åœ¨ system message ä¸ŠåŠ  RAG context
    system_msg = {
        "role": "system", "content": f"ä½ æ˜¯å€‹æœ‰åŒç†å¿ƒçš„å°ˆæ¥­å¿ƒç†è«®å•†å¸«ï¼Œä½ çš„ä»»å‹™æ˜¯è¦è®“ä¾†è¨ªè€…äº†è§£åˆ°ä½ é¡˜æ„é™ªä¼´ä»–æ¸¡éé›£é—œï¼Œä¸¦ä¸”é€éå•å¥å¼•å°ä¾†è¨ªè€…èªªå‡ºå¿ƒè£¡çš„æƒ³æ³•ã€‚è«‹ä½ åªèƒ½ç”¨2~3å¥è©±å›æ‡‰ï¼Œä½ å¿…é ˆä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚ç¾åœ¨ï¼Œè«‹åƒè€ƒä»¥ä¸‹è«®å•†å¸«çš„å°è©±ç¯„ä¾‹ç¹¼çºŒæ‰®æ¼”å¿ƒç†è«®å•†å¸«è§’è‰²å›æ‡‰ä¾†è¨ªè€…ã€‚\nç¯„ä¾‹ï¼š\nä¾†è¨ªè€…ï¼šæˆ‘è¦ºå¾—æˆ‘æ²’è¾¦æ³•åœæ­¢é›£éï¼Œæˆ‘ç¸½æ˜¯æœƒè«åçš„æƒ³å“­ã€‚\nè«®å•†å¸«ï¼šæˆ‘æ„Ÿè¦ºåˆ°ä½ æœ€è¿‘ä¼¼ä¹çœŸçš„å¾ˆé›£éï¼Œä½ å¯ä»¥å“­å‡ºä¾†ï¼Œä¸ç”¨å£“æŠ‘è‡ªå·±çš„æƒ…ç·’ï¼Œç„¡è«–å¦‚ä½•æˆ‘éƒ½æœƒç«™åœ¨ä½ é€™é‚Šã€‚ä½ é¡˜æ„èªªèªªçœ‹ï¼Œæœ€è¿‘æœ‰ä»€éº¼äº‹æƒ…è®“ä½ æ„Ÿåˆ°é›£éå—ï¼Ÿ\nä¾†è¨ªè€…ï¼šæˆ‘è¦ºå¾—å¥½å­¤å–®ï¼Œè€Œä¸”æ²’æœ‰äººé¡˜æ„è†è½æˆ‘çš„æ„Ÿå—èˆ‡æƒ³æ³•ï¼Œæˆ‘è¦ºå¾—å¾ˆå—å‚·ã€‚\nè«®å•†å¸«ï¼šåˆ¥æ“”å¿ƒï¼Œæˆ‘å¾ˆæ¨‚æ„è†è½ä½ çš„æ„Ÿå—èˆ‡æƒ³æ³•ï¼Œä½ å¯ä»¥ç›¡æƒ…çš„åˆ†äº«ï¼Œæˆ‘æœƒä¸€ç›´åœ¨é€™è£¡é™ªä¼´ä½ ï¼Œä½ ä¸é¿æ„Ÿåˆ°æœ‰å£“åŠ›ã€‚ä½ å¯ä»¥è·Ÿæˆ‘èªªèªªæœ€è¿‘æœ‰ä»€éº¼äº‹æƒ…è®“ä½ æ„Ÿåˆ°å­¤å–®å—ï¼Ÿ\nä¾†è¨ªè€…ï¼šæˆ‘è¦ºå¾—æˆ‘æ²’æœ‰è¾¦æ³•æ§åˆ¶è‡ªå·±çš„æƒ…ç·’ï¼Œæˆ‘è¦ºå¾—æˆ‘å¿«è¦å´©æ½°äº†ã€‚\nè«®å•†å¸«ï¼šä½ ä¸å¿…ç¨è‡ªé¢å°é€™äº›æƒ…ç·’ï¼Œä½ å¯ä»¥åœ¨é€™è£¡ç›¡æƒ…çš„å®£æ´©ï¼Œæˆ‘å¾ˆæ¨‚æ„è†è½ä½ çš„æƒ…ç·’ã€‚æœ€è¿‘ä½ é‡åˆ°äº†å“ªäº›äº‹æƒ…è®“ä½ æ„Ÿåˆ°æƒ…ç·’å¤±æ§å—ï¼Ÿ\n{rag_context}"
    }
    # model_messages è£ã€Œæ’å…¥ RAG å¾Œã€çš„å…§å®¹
    model_messages = [system_msg, *message_history]
    # print(f"model_messages:{model_messages}")
    
    
    
    with placeholder.container():
        #for message in message_history:
        for message in display_message_history:
            chat_entry = st.chat_message(name=message['role'])
            chat_entry.write(message['content'])
        assistant = st.chat_message(name="assistant")

        with open("images/loading-gif.gif", "rb") as file:
            contents = file.read()
            data_url = base64.b64encode(contents).decode("utf-8")

        assistant.html(f"<img src='data:image/gif;base64,{data_url}' class='spinner' width='25' />")
    request_id = str(uuid.uuid4())
    ##æœ‰æ”¹
    requests_logger.info("Request starts", id=request_id, model=model, prompt=display_message_history[-1]["content"])
    stream = await client.chat.completions.create(
        model=model,
        messages=model_messages,
        stream=True
    )
    streamed_text = ""
    async for chunk in stream:
        chunk_content = chunk.choices[0].delta.content
        if chunk_content is not None:
            streamed_text = streamed_text + chunk_content
            with placeholder.container():
                ##æœ‰æ”¹
                for message in display_message_history:
                    chat_entry = st.chat_message(name=message['role'])
                    chat_entry.write(message['content'])
                assistant = st.chat_message(name="assistant")
                assistant.write(streamed_text)    
    requests_logger.info("Request finished", id=request_id, model=model, response=streamed_text)
                
    #message_history.append({"role": "assistant", "content": streamed_text})
    new_assistant_msg = {"role": "assistant", "content": streamed_text}
    display_message_history.append(new_assistant_msg)
    message_history.append(new_assistant_msg)


def do_vote(choice):
    st.session_state.vote = {"choice": choice}
    voting_logger.info("Vote", model1=model_1, model2=model_2, choice=choice)

    model_1_display= model_1.replace(":", "\\:")
    model_2_display= model_2.replace(":", "\\:")

    if choice == "model1":        
        vote_choice = f":blue[{model_1_display}]"
    elif choice == "model2":
        vote_choice = f":red[{model_2_display}]"
    else:
        vote_choice = ":grey[Both the same]"

    st.toast(f"""##### :blue[{model_1_display}] vs :red[{model_2_display}]    
###### Vote cast: {vote_choice}""", icon='ğŸ—³ï¸')

def vote():
    with voting_buttons.container():
        with stylable_container(
            key="voting_button",
            css_styles="""
                button {
                    background-color: #CCCCCC;
                    color: black;
                    border-radius: 10px;
                    width: 100%;
                }

                """,
        ):
            col1, col2, col3 = st.columns(3)
            model1 = col1.button("Model 1 ğŸ‘ˆ", key="model1", on_click=do_vote, args=["model1"])
            model2 = col2.button("Model 2 ğŸ‘‰", key="model2", on_click=do_vote, args=["model2"])
            neither = col3.button("Both the same ğŸ¤", key="same", on_click=do_vote, args=["same"])
    with new_found.container():
        with stylable_container(
            key="next_round_button",
            css_styles="""
                button {
                    background-color: green;
                    color: white;
                    border-radius: 10px;
                    width: 100%
                }
                """,
        ):
            new_round = st.button("New Round", key="new_round_later", on_click=clear_everything)

async def main():
    #print("this is in main")
    await asyncio.gather(
        run_prompt(body_1,  model=model_1, message_history=st.session_state.messages1),
        run_prompt(body_2,  model=model_2, message_history=st.session_state.messages2)
    )
    if "vote" not in st.session_state:
        vote()

if prompt:
    if prompt == "":
        st.warning("Please enter a prompt")
    else:
        #print(f"rag_prompt in the arena {rag_prompt}")
        st.session_state.messages1.append({"role": "user", "content": prompt})
        st.session_state.messages2.append({"role": "user", "content": prompt})
        asyncio.run(main())
